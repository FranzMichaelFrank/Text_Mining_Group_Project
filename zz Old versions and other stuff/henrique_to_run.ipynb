{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82674cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651db2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = \"/Users/philippmetzger/OneDrive/PHILIPP/NOVA IMS/2nd Semester/06 Text Mining 4 ECTS/00 Project/corpus/\"\n",
    "cs_en = pd.read_csv(path_corpus + \"cs-en/scores.csv\")\n",
    "de_en = pd.read_csv(path_corpus + \"de-en/scores.csv\")\n",
    "en_fi = pd.read_csv(path_corpus + \"en-fi/scores.csv\")\n",
    "en_zh = pd.read_csv(path_corpus + \"en-zh/scores.csv\")\n",
    "ru_en = pd.read_csv(path_corpus + \"ru-en/scores.csv\")\n",
    "zh_en = pd.read_csv(path_corpus + \"zh-en/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246c1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [cs_en, de_en, en_fi, en_zh, ru_en, zh_en]\n",
    "names_list = ['cs_en', 'de_en', 'en_fi', 'en_zh', 'ru_en', 'zh_en']\n",
    "text_columns_list = ['source', 'reference', 'translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29a12d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d72e47650a445808d3fa75dbc94e307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for column in tqdm(range(3)):\n",
    "    for data in data_list:\n",
    "        for i in range(data.shape[0]):\n",
    "            string = data.iloc[i,column].lower()\n",
    "            string = string.replace('.', ' .')\n",
    "            string = string.replace('?', ' .')\n",
    "            string = string.replace('!', ' .')\n",
    "            string = string.replace('(', '')\n",
    "            string = string.replace(')', '')\n",
    "            # Remove all numbers\n",
    "            string = re.sub(r'[0-9]', '', string)\n",
    "            # If string contains chinese\n",
    "            if re.search(u'[\\u4e00-\\u9fff]', string):\n",
    "                string = list(string)\n",
    "                string = ' '.join(string)\n",
    "            split_entry = string.split()\n",
    "            tokenized_corpus.append(split_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64fba7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {word for doc in tokenized_corpus for word in doc}\n",
    "word2idx = {w:idx for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2b4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training(tokenized_corpus, word2idx, window_size=2):\n",
    "    window_size = 2\n",
    "    idx_pairs = []\n",
    "    \n",
    "    # for each sentence\n",
    "    for sentence in tqdm(tokenized_corpus):\n",
    "        indices = [word2idx[word] for word in sentence]\n",
    "        # for each word, threated as center word\n",
    "        for center_word_pos in range(len(indices)):\n",
    "            # for each window position\n",
    "            for w in range(-window_size, window_size + 1):\n",
    "                context_word_pos = center_word_pos + w\n",
    "                # make soure not jump out sentence\n",
    "                if  context_word_pos < 0 or \\\n",
    "                    context_word_pos >= len(indices) or \\\n",
    "                    center_word_pos == context_word_pos:\n",
    "                    continue  \n",
    "                context_word_idx = indices[context_word_pos]\n",
    "                idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "    return np.array(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce99d72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bcd0d93d4d4abd9e608e121176c721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/283971 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_pairs = build_training(tokenized_corpus, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ec63a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_vector(word_idx, vocabulary):\n",
    "    x = torch.zeros(len(vocabulary)).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "def Skip_Gram(training_pairs, vocabulary, embedding_dims=5, learning_rate=0.001, epochs=10):\n",
    "    torch.manual_seed(3)\n",
    "    W1 = Variable(torch.randn(embedding_dims, len(vocabulary)).float(), requires_grad=True)\n",
    "    losses = []\n",
    "    for epo in tqdm(range(epochs)):\n",
    "        loss_val = 0\n",
    "        for input_word, target in training_pairs:\n",
    "            x = Variable(get_onehot_vector(input_word, vocabulary)).float()\n",
    "            y_true = Variable(torch.from_numpy(np.array([target])).long())\n",
    "\n",
    "            # Matrix multiplication to obtain the input word embedding\n",
    "            z1 = torch.matmul(W1, x)\n",
    "    \n",
    "            # Matrix multiplication to obtain the z score for each word\n",
    "            z2 = torch.matmul(torch.transpose(W1, 0, 1), z1)\n",
    "    \n",
    "            # Apply Log and softmax functions\n",
    "            log_softmax = F.log_softmax(z2, dim=0)\n",
    "            # Compute the negative-log-likelihood loss\n",
    "            loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "            loss_val += loss.item()\n",
    "            \n",
    "            # compute the gradient in function of the error\n",
    "            loss.backward() \n",
    "            \n",
    "            # Update your embeddings\n",
    "            W1.data -= learning_rate * W1.grad.data\n",
    "            #W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "            W1.grad.data.zero_()\n",
    "            #W2.grad.data.zero_()\n",
    "        \n",
    "        losses.append(loss_val/len(training_pairs))\n",
    "    \n",
    "    return W1, losses\n",
    "\n",
    "def plot_loss(loss):\n",
    "    x_axis = [epoch+1 for epoch in range(len(loss))]\n",
    "    plt.plot(x_axis, loss, '-g', linewidth=1, label='Train')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff53390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bd2fc657ae400180f200fb024363df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b123f7cb03f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkip_Gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-136421845a27>\u001b[0m in \u001b[0;36mSkip_Gram\u001b[0;34m(training_pairs, vocabulary, embedding_dims, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# compute the gradient in function of the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Update your embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tm_conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tm_conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, losses = Skip_Gram(training_pairs, word2idx, epochs=1)\n",
    "#W1, losses = Skip_Gram(training_pairs, word2idx, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_loss(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
