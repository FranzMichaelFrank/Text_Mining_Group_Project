{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aboriginal-contamination",
   "metadata": {},
   "source": [
    "# TM Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-thomas",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "\n",
    "path_corpus = \"/Users/franz/Desktop/TM Project/corpus/\"\n",
    "\n",
    "ru_en = pd.read_csv(path_corpus + \"ru-en/scores.csv\")\n",
    "de_en = pd.read_csv(path_corpus + \"de-en/scores.csv\")\n",
    "cs_en = pd.read_csv(path_corpus + \"cs-en/scores.csv\")\n",
    "zh_en = pd.read_csv(path_corpus + \"zh-en/scores.csv\")\n",
    "en_zh = pd.read_csv(path_corpus + \"en-zh/scores.csv\")\n",
    "en_fi = pd.read_csv(path_corpus + \"en-fi/scores.csv\")\n",
    "\n",
    "ru_en_ = ru_en.copy()\n",
    "de_en_ = de_en.copy()\n",
    "cs_en_ = cs_en.copy()\n",
    "zh_en_ = zh_en.copy()\n",
    "en_zh_ = en_zh.copy()\n",
    "en_fi_ = en_fi.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-fitness",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [\"Russian into English\", \"German into English\", \"Czech into English\", \"Chinese into English\", \"English into Chinese\", \"English into Finish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "zscores = []\n",
    "avgscores = []\n",
    "annots = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    rows.append(element.shape[0])\n",
    "    zscores.append(np.round(element[\"z-score\"].mean(),2))\n",
    "    avgscores.append(np.round(element[\"avg-score\"].mean(), 2))\n",
    "    annots.append(np.round(element[\"annotators\"].mean(),2))\n",
    "    i += 1                   \n",
    "    \n",
    "exploration_df = pd.DataFrame([rows, zscores, avgscores, annots]).T.rename(columns={0:\"rows\", 1:\"avg z-score\", 2:\"avg avg-score\", 3:\"avg annotators\"})\n",
    "exploration_df[\"description\"] = descriptions\n",
    "exploration_df = exploration_df.set_index(\"description\")\n",
    "exploration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-president",
   "metadata": {},
   "source": [
    "As there are only 6 different types of translations, these correlations might be not very meaningful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-intellectual",
   "metadata": {},
   "source": [
    "# Lexical metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-contract",
   "metadata": {},
   "source": [
    "## BLEU Score - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# a more \"pythonic\" way to compute BLUE_star \n",
    "\n",
    "def BLEU_star_compact(refs, candidate):\n",
    "    refs = [refs.split()]\n",
    "    candidate = candidate.split()\n",
    "\n",
    "    return sum([min(count, max([ref[word] for ref in [Counter(ref) for ref in refs]])) for word, count in Counter(candidate).items()])/len(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = {}\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    bleu_scores = []\n",
    "\n",
    "    #calculating the bleu scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        reference = element.loc[i,\"reference\"]\n",
    "        translation = element.loc[i,\"translation\"]\n",
    "        bleu_scores.append(BLEU_star_compact(reference, translation))\n",
    "\n",
    "    #add the bleu scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"BLEU\"] = bleu_scores\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"BLEU\"] = bleu_scores\n",
    "    elif co == 1:\n",
    "        de_en_[\"BLEU\"] = bleu_scores\n",
    "    elif co == 2:\n",
    "        cs_en_[\"BLEU\"] = bleu_scores\n",
    "    elif co == 3:\n",
    "        zh_en_[\"BLEU\"] = bleu_scores\n",
    "    elif co == 4:\n",
    "        en_zh_[\"BLEU\"] = bleu_scores\n",
    "    elif co == 5:\n",
    "        en_fi_[\"BLEU\"] = bleu_scores\n",
    "    co += 1\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\033[1mCorrelation between z-score and BLEU score\\n\")\n",
    "i = 0\n",
    "for element in correlations_p:\n",
    "    print(\"\\033[1m\", descriptions[i] + \":\",  \"\\033[0mPearson:\", np.round(element,4), \"| Kendall:\", np.round(correlations_k[i],4))\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n\\033[1mOverall:\\033[0m Average Pearson:\", np.round(sum(correlations_p)/len(correlations_p),4),\n",
    "         \"| Average Kendall:\", np.round(sum(correlations_k)/len(correlations_k),4))\n",
    "\n",
    "overall_results[\"BLEU Star Pearson\"] = correlations_p\n",
    "overall_results[\"BLEU Star Kendall\"] = correlations_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-explanation",
   "metadata": {},
   "source": [
    "## BLEU Score - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-desperate",
   "metadata": {},
   "source": [
    "### 1st Try (sentence_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    bleu_scores = []\n",
    "\n",
    "    #calculating the bleu scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        reference = [element.loc[i,\"reference\"].split()]\n",
    "        translation = element.loc[i,\"translation\"].split()\n",
    "        bleu_scores.append(sentence_bleu(reference, translation,weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    #add the bleu scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"BLEU\"] = bleu_scores\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"BLEU_s\"] = bleu_scores\n",
    "    elif co == 1:\n",
    "        de_en_[\"BLEU_s\"] = bleu_scores\n",
    "    elif co == 2:\n",
    "        cs_en_[\"BLEU_s\"] = bleu_scores\n",
    "    elif co == 3:\n",
    "        zh_en_[\"BLEU_s\"] = bleu_scores\n",
    "    elif co == 4:\n",
    "        en_zh_[\"BLEU_s\"] = bleu_scores\n",
    "    elif co == 5:\n",
    "        en_fi_[\"BLEU_s\"] = bleu_scores\n",
    "\n",
    "    co += 1\n",
    "    \n",
    "print(\"\\033[1mCorrelation between z-score and BLEU score\\n\")\n",
    "i = 0\n",
    "for element in correlations_p:\n",
    "    print(\"\\033[1m\", descriptions[i] + \":\",  \"\\033[0mPearson:\", np.round(element,4), \"| Kendall:\", np.round(correlations_k[i],4))\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n\\033[1mOverall:\\033[0m Average Pearson:\", np.round(sum(correlations_p)/len(correlations_p),4),\n",
    "         \"| Average Kendall:\", np.round(sum(correlations_k)/len(correlations_k),4))\n",
    "\n",
    "overall_results[\"BLEU Sentence Pearson\"] = correlations_p\n",
    "overall_results[\"BLEU Sentence Kendall\"] = correlations_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-amber",
   "metadata": {},
   "source": [
    "### 2nd Try (corpus_bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    bleu_scores = []\n",
    "\n",
    "    #calculating the bleu scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        reference = [element.loc[i,\"reference\"].split()]\n",
    "        translation = element.loc[i,\"translation\"].split()\n",
    "        while len(reference) < len(translation):\n",
    "            reference.append(\" \")\n",
    "        while len(reference) > len(translation):\n",
    "            translation.append(\" \")\n",
    "        bleu_scores.append(corpus_bleu(reference, translation))\n",
    "\n",
    "    #add the bleu scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"BLEU\"] = bleu_scores\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"BLEU_c\"] = bleu_scores\n",
    "    elif co == 1:\n",
    "        de_en_[\"BLEU_c\"] = bleu_scores\n",
    "    elif co == 2:\n",
    "        cs_en_[\"BLEU_c\"] = bleu_scores\n",
    "    elif co == 3:\n",
    "        zh_en_[\"BLEU_c\"] = bleu_scores\n",
    "    elif co == 4:\n",
    "        en_zh_[\"BLEU_c\"] = bleu_scores\n",
    "    elif co == 5:\n",
    "        en_fi_[\"BLEU_c\"] = bleu_scores\n",
    "    co += 1\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(\"\\033[1mCorrelation between z-score and BLEU score\\n\")\n",
    "i = 0\n",
    "for element in correlations_p:\n",
    "    print(\"\\033[1m\", descriptions[i] + \":\",  \"\\033[0mPearson\", np.round(element,4), \"| Kendall:\", np.round(correlations_k[i],4))\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n\\033[1mOverall:\\033[0m Average Pearson:\", np.round(sum(correlations_p)/len(correlations_p),4),\n",
    "         \"| Average Kendall:\", np.round(sum(correlations_k)/len(correlations_k),4))\n",
    "\n",
    "overall_results[\"BLEU Corpus Pearson\"] = correlations_p\n",
    "overall_results[\"BLEU Corpus Kendall\"] = correlations_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-divide",
   "metadata": {},
   "source": [
    "## ROUGE Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-donor",
   "metadata": {},
   "source": [
    "### ROUGE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "results_p = pd.DataFrame()\n",
    "results_k = pd.DataFrame()\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "j = 0\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fmeasures = []\n",
    "\n",
    "    #calculating the rouge scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "        precisions.append(scores[\"rouge1\"].precision)\n",
    "        recalls.append(scores[\"rouge1\"].recall)\n",
    "        fmeasures.append(scores[\"rouge1\"].fmeasure)\n",
    "\n",
    "    #add the rouge scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"ROUGE precision\"] = precisions\n",
    "    development_df[\"ROUGE recall\"] = recalls\n",
    "    development_df[\"ROUGE fmeasure\"] = fmeasures\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    col_name = descriptions[j]\n",
    "    j += 1\n",
    "    results_p[col_name] = pd.Series(development_df.corr(method=\"pearson\").iloc[0,3:])\n",
    "    results_k[col_name] = pd.Series(development_df.corr(method=\"kendall\").iloc[0,3:])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"ROUGE1_precision\"] = precisions\n",
    "        ru_en_[\"ROUGE1_recall\"] = recalls\n",
    "        ru_en_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    elif co == 1:\n",
    "        de_en_[\"ROUGE1_precision\"] = precisions\n",
    "        de_en_[\"ROUGE1_recall\"] = recalls\n",
    "        de_en_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    elif co == 2:\n",
    "        cs_en_[\"ROUGE1_precision\"] = precisions\n",
    "        cs_en_[\"ROUGE1_recall\"] = recalls\n",
    "        cs_en_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    elif co == 3:\n",
    "        zh_en_[\"ROUGE1_precision\"] = precisions\n",
    "        zh_en_[\"ROUGE1_recall\"] = recalls\n",
    "        zh_en_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    elif co == 4:\n",
    "        en_zh_[\"ROUGE1_precision\"] = precisions\n",
    "        en_zh_[\"ROUGE1_recall\"] = recalls\n",
    "        en_zh_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    elif co == 5:\n",
    "        en_fi_[\"ROUGE1_precision\"] = precisions\n",
    "        en_fi_[\"ROUGE1_recall\"] = recalls\n",
    "        en_fi_[\"ROUGE1_fmeasure\"] = fmeasures\n",
    "    co += 1\n",
    "\n",
    "\n",
    "print(\"\\033[1mPearson Correlation between z-score and ROUGE measures (ROUGE 1) \\n\")\n",
    "\n",
    "\n",
    "overall_results[\"ROUGE 1 Precision Pearson\"] = results_p.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE 1 Recall Pearson\"] = results_p.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE 1 Fmeasure Pearson\"] = results_p.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_p[\"Average\"] = results_p.T.mean()\n",
    "results_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mKendall Tau Correlation between z-score and ROUGE measures (ROUGE 1) \\n\")\n",
    "\n",
    "overall_results[\"ROUGE 1 Precision Kendall\"] = results_k.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE 1 Recall Kendall\"] = results_k.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE 1 Fmeasure Kendall\"] = results_k.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_k[\"Average\"] = results_k.T.mean()\n",
    "results_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-times",
   "metadata": {},
   "source": [
    "### ROUGE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "results_p = pd.DataFrame()\n",
    "results_k = pd.DataFrame()\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "j = 0\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fmeasures = []\n",
    "\n",
    "    #calculating the rouge scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "        precisions.append(scores[\"rouge2\"].precision)\n",
    "        recalls.append(scores[\"rouge2\"].recall)\n",
    "        fmeasures.append(scores[\"rouge2\"].fmeasure)\n",
    "\n",
    "    #add the rouge scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"ROUGE precision\"] = precisions\n",
    "    development_df[\"ROUGE recall\"] = recalls\n",
    "    development_df[\"ROUGE fmeasure\"] = fmeasures\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    col_name = descriptions[j]\n",
    "    j += 1\n",
    "    results_p[col_name] = pd.Series(development_df.corr(method=\"pearson\").iloc[0,3:])\n",
    "    results_k[col_name] = pd.Series(development_df.corr(method=\"kendall\").iloc[0,3:])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"ROUGE2_precision\"] = precisions\n",
    "        ru_en_[\"ROUGE2_recall\"] = recalls\n",
    "        ru_en_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    elif co == 1:\n",
    "        de_en_[\"ROUGE2_precision\"] = precisions\n",
    "        de_en_[\"ROUGE2_recall\"] = recalls\n",
    "        de_en_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    elif co == 2:\n",
    "        cs_en_[\"ROUGE2_precision\"] = precisions\n",
    "        cs_en_[\"ROUGE2_recall\"] = recalls\n",
    "        cs_en_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    elif co == 3:\n",
    "        zh_en_[\"ROUGE2_precision\"] = precisions\n",
    "        zh_en_[\"ROUGE2_recall\"] = recalls\n",
    "        zh_en_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    elif co == 4:\n",
    "        en_zh_[\"ROUGE2_precision\"] = precisions\n",
    "        en_zh_[\"ROUGE2_recall\"] = recalls\n",
    "        en_zh_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    elif co == 5:\n",
    "        en_fi_[\"ROUGE2_precision\"] = precisions\n",
    "        en_fi_[\"ROUGE2_recall\"] = recalls\n",
    "        en_fi_[\"ROUGE2_fmeasure\"] = fmeasures\n",
    "    co += 1\n",
    "\n",
    "\n",
    "print(\"\\033[1mPearson Correlation between z-score and ROUGE measures (ROUGE 2) \\n\")\n",
    "\n",
    "overall_results[\"ROUGE 2 Precision Pearson\"] = results_p.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE 2 Recall Pearson\"] = results_p.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE 2 Fmeasure Pearson\"] = results_p.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_p[\"Average\"] = results_p.T.mean()\n",
    "results_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mKendall Tau Correlation between z-score and ROUGE measures (ROUGE 2) \\n\")\n",
    "\n",
    "overall_results[\"ROUGE 2 Precision Kendall\"] = results_k.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE 2 Recall Kendall\"] = results_k.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE 2 Fmeasure Kendall\"] = results_k.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_k[\"Average\"] = results_k.T.mean()\n",
    "results_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-valley",
   "metadata": {},
   "source": [
    "### ROUGE L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "results_p = pd.DataFrame()\n",
    "results_k = pd.DataFrame()\n",
    "\n",
    "correlations_p = []\n",
    "correlations_k = []\n",
    "j = 0\n",
    "co = 0\n",
    "for element in [ru_en, de_en, cs_en, zh_en, en_zh, en_fi]:\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    fmeasures = []\n",
    "\n",
    "    #calculating the rouge scores for the translations in comparison to their respective reference\n",
    "    for i in range(element.shape[0]): #element.shape[0]\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "        precisions.append(scores[\"rougeL\"].precision)\n",
    "        recalls.append(scores[\"rougeL\"].recall)\n",
    "        fmeasures.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "    #add the rouge scores to the dataframe\n",
    "    development_df = element.copy() #element.shape[0]\n",
    "    development_df[\"ROUGE precision\"] = precisions\n",
    "    development_df[\"ROUGE recall\"] = recalls\n",
    "    development_df[\"ROUGE fmeasure\"] = fmeasures\n",
    "    correlations_p.append(development_df.corr(method=\"pearson\").iloc[-1:,0].values[0])\n",
    "    correlations_k.append(development_df.corr(method=\"kendall\").iloc[-1:,0].values[0])\n",
    "    col_name = descriptions[j]\n",
    "    j += 1\n",
    "    results_p[col_name] = pd.Series(development_df.corr(method=\"pearson\").iloc[0,3:])\n",
    "    results_k[col_name] = pd.Series(development_df.corr(method=\"kendall\").iloc[0,3:])\n",
    "    \n",
    "    if co == 0:\n",
    "        ru_en_[\"ROUGEL_precision\"] = precisions\n",
    "        ru_en_[\"ROUGEL_recall\"] = recalls\n",
    "        ru_en_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    elif co == 1:\n",
    "        de_en_[\"ROUGEL_precision\"] = precisions\n",
    "        de_en_[\"ROUGEL_recall\"] = recalls\n",
    "        de_en_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    elif co == 2:\n",
    "        cs_en_[\"ROUGEL_precision\"] = precisions\n",
    "        cs_en_[\"ROUGEL_recall\"] = recalls\n",
    "        cs_en_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    elif co == 3:\n",
    "        zh_en_[\"ROUGEL_precision\"] = precisions\n",
    "        zh_en_[\"ROUGEL_recall\"] = recalls\n",
    "        zh_en_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    elif co == 4:\n",
    "        en_zh_[\"ROUGEL_precision\"] = precisions\n",
    "        en_zh_[\"ROUGEL_recall\"] = recalls\n",
    "        en_zh_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    elif co == 5:\n",
    "        en_fi_[\"ROUGEL_precision\"] = precisions\n",
    "        en_fi_[\"ROUGEL_recall\"] = recalls\n",
    "        en_fi_[\"ROUGEL_fmeasure\"] = fmeasures\n",
    "    co += 1\n",
    "    \n",
    "\n",
    "\n",
    "print(\"\\033[1mPearson Correlation between z-score and ROUGE measures (ROUGE L) \\n\")\n",
    "\n",
    "overall_results[\"ROUGE L Precision Pearson\"] = results_p.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE L Recall Pearson\"] = results_p.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE L Fmeasure Pearson\"] = results_p.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_p[\"Average\"] = results_p.T.mean()\n",
    "results_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1mKendall Tau Correlation between z-score and ROUGE measures (ROUGE L) \\n\")\n",
    "\n",
    "overall_results[\"ROUGE L Precision Kendall\"] = results_k.iloc[0,:6].values.tolist()\n",
    "overall_results[\"ROUGE L Recall Kendall\"] = results_k.iloc[1,:6].values.tolist()\n",
    "overall_results[\"ROUGE L Fmeasure Kendall\"] = results_k.iloc[2,:6].values.tolist()\n",
    "\n",
    "results_k[\"Average\"] = results_k.T.mean()\n",
    "results_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-pension",
   "metadata": {},
   "source": [
    "## RESULTS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsons = []\n",
    "kendalls = []\n",
    "\n",
    "for element in list(overall_results.keys()):\n",
    "    if element.endswith(\"Pearson\"):\n",
    "        pearsons.append(element)\n",
    "    elif element.endswith(\"Kendall\"):\n",
    "        kendalls.append(element)\n",
    "        \n",
    "dict_pearson = { your_key: overall_results[your_key] for your_key in pearsons }\n",
    "dict_kendall = { your_key: overall_results[your_key] for your_key in kendalls }\n",
    "\n",
    "pearson_df = pd.DataFrame(list(dict_pearson.values()), index=list(dict_pearson.keys()), columns=descriptions)\n",
    "pearson_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_evaluation = pd.DataFrame(pearson_df.idxmax(), columns=[\"Metric with highest correlation\"])\n",
    "pearson_evaluation[\"Value\"] = pearson_df.max()\n",
    "pearson_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "kendall_df = pd.DataFrame(list(dict_kendall.values()), index=list(dict_kendall.keys()), columns=descriptions)\n",
    "kendall_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "kendall_evaluation = pd.DataFrame(kendall_df.idxmax(), columns=[\"Metric with highest correlation\"])\n",
    "kendall_evaluation[\"Value\"] = kendall_df.max()\n",
    "kendall_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-institute",
   "metadata": {},
   "source": [
    "## COMBINATION - Predicting the scores for the testset with the best respective metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "\n",
    "path_corpus_test = \"/Users/franz/Downloads/testset/\"\n",
    "\n",
    "ru_en_test = pd.read_csv(path_corpus_test + \"ru-en/scores.csv\")\n",
    "de_en_test = pd.read_csv(path_corpus_test + \"de-en/scores.csv\")\n",
    "cs_en_test = pd.read_csv(path_corpus_test + \"cs-en/scores.csv\")\n",
    "zh_en_test = pd.read_csv(path_corpus_test + \"zh-en/scores.csv\")\n",
    "en_zh_test = pd.read_csv(path_corpus_test + \"en-zh/scores.csv\")\n",
    "en_fi_test = pd.read_csv(path_corpus_test + \"en-fi/scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-flooring",
   "metadata": {},
   "source": [
    "Throughout all the test set, there is only one field containing a nan, which is ru_en_test.iloc[9191,1]. As the metric score cannot be computed without a reference, this row has to be deleted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "j = 0\n",
    "for element in ru_en_test.isna()[\"reference\"].tolist():\n",
    "    if element == True:\n",
    "        j = i\n",
    "    i +=1\n",
    "    \n",
    "pd.DataFrame(ru_en_test.iloc[j,:]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_test = ru_en_test.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "for element in [ru_en_test, de_en_test, cs_en_test, zh_en_test, en_zh_test, en_fi_test]:\n",
    "    \n",
    "    # detect the language pair contained in the dataframe\n",
    "    pair = detect(element.iloc[0,0]) + \"_\" + detect(element.iloc[0,1])\n",
    "    \n",
    "    predicted_scores = []\n",
    "    \n",
    "    if pair in [\"ru_en\", \"cs_en\", \"de_en\", \"zh-cn_en\", \"en_fi\"]:\n",
    "        for i in range(element.shape[0]):\n",
    "            scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "            scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "            if pair in [\"ru_en\", \"cs_en\"]:\n",
    "                predicted_scores.append(scores[\"rougeL\"].precision)\n",
    "            elif pair == \"de_en\":\n",
    "                predicted_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "            elif pair in [\"zh-cn_en\", \"en_fi\"]:\n",
    "                predicted_scores.append(scores[\"rouge1\"].precision)\n",
    "                \n",
    "    elif pair == \"en_zh-cn\":\n",
    "        for i in range(element.shape[0]):\n",
    "            reference = [element.loc[i,\"reference\"].split()]\n",
    "            translation = element.loc[i,\"translation\"].split()\n",
    "            while len(reference) < len(translation):\n",
    "                reference.append(\" \")\n",
    "            while len(reference) > len(translation):\n",
    "                translation.append(\" \")\n",
    "            predicted_scores.append(corpus_bleu(reference, translation))\n",
    "        \n",
    "            \n",
    "    element[\"predicted_score\"] = predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in [ru_en_test, de_en_test, cs_en_test, zh_en_test, en_zh_test, en_fi_test]:\n",
    "    display(element.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-sessions",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION ON RESPECTIVE TOP METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_corpus = \"/Users/franz/Desktop/TM Project/corpus/\"\n",
    "\n",
    "ru_en = pd.read_csv(path_corpus + \"ru-en/scores.csv\")\n",
    "de_en = pd.read_csv(path_corpus + \"de-en/scores.csv\")\n",
    "cs_en = pd.read_csv(path_corpus + \"cs-en/scores.csv\")\n",
    "zh_en = pd.read_csv(path_corpus + \"zh-en/scores.csv\")\n",
    "en_zh = pd.read_csv(path_corpus + \"en-zh/scores.csv\")\n",
    "en_fi = pd.read_csv(path_corpus + \"en-fi/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 3 metrics by language pair\n",
    "for i in range(6):\n",
    "    display(pd.DataFrame(pearson_df.iloc[:,i].sort_values(ascending = False).head(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-smith",
   "metadata": {},
   "source": [
    "### Russian into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_precision = []\n",
    "l_fmeasure = []\n",
    "precision_1 = []\n",
    "fmeasure_1 = []\n",
    "bleu_star = []\n",
    "\n",
    "for i in range(ru_en.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(ru_en.loc[i,\"reference\"], ru_en.loc[i,\"translation\"])\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "\n",
    "    reference = ru_en.loc[i,\"reference\"]\n",
    "    translation = ru_en.loc[i,\"translation\"]\n",
    "    bleu_star.append(BLEU_star_compact(reference, translation))\n",
    "    \n",
    "ru_en[\"l_precision\"] = l_precision\n",
    "ru_en[\"l_fmeasure\"] = l_fmeasure\n",
    "ru_en[\"precision_1\"] = precision_1\n",
    "ru_en[\"fmeasure_1\"] = fmeasure_1\n",
    "ru_en[\"bleu_star\"] = bleu_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_scores = ru_en[[\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\", \"z-score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "for element in [\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\"]:\n",
    "    ru_en_scores[element] = zscore(ru_en_scores[element])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_train = ru_en_scores.iloc[:int(ru_en_scores.shape[0]*0.7),:]\n",
    "ru_en_test = ru_en_scores.iloc[int(ru_en_scores.shape[0]*0.7):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = ru_en_train.iloc[:,:-1]\n",
    "Y = ru_en_train.iloc[:,-1]\n",
    "X_test = ru_en_test.iloc[:,:-1]\n",
    "Y_test = ru_en_test.iloc[:,-1]\n",
    "\n",
    "model_ru_en = LinearRegression()\n",
    "model_ru_en.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_ru_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-bibliography",
   "metadata": {},
   "source": [
    "### German into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fmeasure = []\n",
    "fmeasure_1 = []\n",
    "l_precision = []\n",
    "l_recall = []\n",
    "precision_1 = []\n",
    "\n",
    "\n",
    "for i in range(de_en.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(de_en.loc[i,\"reference\"], de_en.loc[i,\"translation\"])\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    l_recall.append(scores[\"rougeL\"].recall)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    \n",
    "de_en[\"l_fmeasure\"] = l_fmeasure\n",
    "de_en[\"fmeasure_1\"] = fmeasure_1\n",
    "de_en[\"l_precision\"] = l_precision\n",
    "de_en[\"l_recall\"] = l_recall\n",
    "de_en[\"precision_1\"] = precision_1\n",
    "\n",
    "de_en_scores = de_en[[\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"l_recall\", \"precision_1\", \"z-score\"]]\n",
    "\n",
    "for element in [\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"l_recall\", \"precision_1\"]:\n",
    "    de_en_scores[element] = zscore(de_en_scores[element])\n",
    "    \n",
    "de_en_train = de_en_scores.iloc[:int(de_en_scores.shape[0]*0.7),:]\n",
    "de_en_test = de_en_scores.iloc[int(de_en_scores.shape[0]*0.7):,:]\n",
    "\n",
    "X = de_en_train.iloc[:,:-1]\n",
    "Y = de_en_train.iloc[:,-1]\n",
    "X_test = de_en_test.iloc[:,:-1]\n",
    "Y_test = de_en_test.iloc[:,-1]\n",
    "\n",
    "model_de_en = LinearRegression()\n",
    "model_de_en.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_de_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fmeasure = []\n",
    "fmeasure_1 = []\n",
    "l_precision = []\n",
    "l_recall = []\n",
    "precision_1 = []\n",
    "\n",
    "\n",
    "for i in range(de_en.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(de_en.loc[i,\"reference\"], de_en.loc[i,\"translation\"])\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    l_recall.append(scores[\"rougeL\"].recall)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    \n",
    "de_en[\"l_fmeasure\"] = l_fmeasure\n",
    "de_en[\"fmeasure_1\"] = fmeasure_1\n",
    "de_en[\"l_precision\"] = l_precision\n",
    "de_en[\"l_recall\"] = l_recall\n",
    "de_en[\"precision_1\"] = precision_1\n",
    "\n",
    "de_en_scores = de_en[[\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"l_recall\", \"precision_1\", \"z-score\"]]\n",
    "\n",
    "for element in [\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"l_recall\", \"precision_1\"]:\n",
    "    de_en_scores[element] = zscore(de_en_scores[element])\n",
    "    \n",
    "de_en_train = de_en_scores.iloc[:int(de_en_scores.shape[0]*0.7),:]\n",
    "de_en_test = de_en_scores.iloc[int(de_en_scores.shape[0]*0.7):,:]\n",
    "\n",
    "X = de_en_train.iloc[:,:-1]\n",
    "Y = de_en_train.iloc[:,-1]\n",
    "X_test = de_en_test.iloc[:,:-1]\n",
    "Y_test = de_en_test.iloc[:,-1]\n",
    "\n",
    "model_de_en = LinearRegression()\n",
    "model_de_en.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_de_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-performance",
   "metadata": {},
   "source": [
    "### Czech into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_precision = []\n",
    "l_fmeasure = []\n",
    "precision_1 = []\n",
    "fmeasure_1 = []\n",
    "bleu_star = []\n",
    "\n",
    "for i in range(cs_en.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(cs_en.loc[i,\"reference\"], cs_en.loc[i,\"translation\"])\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "\n",
    "    reference = cs_en.loc[i,\"reference\"]\n",
    "    translation = cs_en.loc[i,\"translation\"]\n",
    "    bleu_star.append(BLEU_star_compact(reference, translation))\n",
    "    \n",
    "cs_en[\"l_precision\"] = l_precision\n",
    "cs_en[\"l_fmeasure\"] = l_fmeasure\n",
    "cs_en[\"precision_1\"] = precision_1\n",
    "cs_en[\"fmeasure_1\"] = fmeasure_1\n",
    "cs_en[\"bleu_star\"] = bleu_star\n",
    "\n",
    "cs_en_scores = cs_en[[\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\", \"z-score\"]]\n",
    "\n",
    "for element in [\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\"]:\n",
    "    cs_en_scores[element] = zscore(cs_en_scores[element])\n",
    "    \n",
    "cs_en_train = cs_en_scores.iloc[:int(cs_en_scores.shape[0]*0.7),:]\n",
    "cs_en_test = cs_en_scores.iloc[int(cs_en_scores.shape[0]*0.7):,:]\n",
    "\n",
    "X = cs_en_train.iloc[:,:-1]\n",
    "Y = cs_en_train.iloc[:,-1]\n",
    "X_test = cs_en_test.iloc[:,:-1]\n",
    "Y_test = cs_en_test.iloc[:,-1]\n",
    "X_full = cs_en_scores.iloc[:,:-1]\n",
    "Y_full = cs_en_scores.iloc[:,-1]\n",
    "\n",
    "model_cs_en = LinearRegression()\n",
    "model_cs_en.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_cs_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-reputation",
   "metadata": {},
   "source": [
    "### Chinese into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_precision = []\n",
    "l_fmeasure = []\n",
    "precision_1 = []\n",
    "fmeasure_1 = []\n",
    "bleu_star = []\n",
    "\n",
    "for i in range(zh_en.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(zh_en.loc[i,\"reference\"], zh_en.loc[i,\"translation\"])\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "\n",
    "    reference = zh_en.loc[i,\"reference\"]\n",
    "    translation = zh_en.loc[i,\"translation\"]\n",
    "    bleu_star.append(BLEU_star_compact(reference, translation))\n",
    "    \n",
    "zh_en[\"l_precision\"] = l_precision\n",
    "zh_en[\"l_fmeasure\"] = l_fmeasure\n",
    "zh_en[\"precision_1\"] = precision_1\n",
    "zh_en[\"fmeasure_1\"] = fmeasure_1\n",
    "zh_en[\"bleu_star\"] = bleu_star\n",
    "\n",
    "zh_en_scores = zh_en[[\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\", \"z-score\"]]\n",
    "\n",
    "for element in [\"l_precision\", \"l_fmeasure\", \"precision_1\", \"fmeasure_1\", \"bleu_star\"]:\n",
    "    zh_en_scores[element] = zscore(zh_en_scores[element])\n",
    "    \n",
    "zh_en_train = zh_en_scores.iloc[:int(zh_en_scores.shape[0]*0.7),:]\n",
    "zh_en_test = zh_en_scores.iloc[int(zh_en_scores.shape[0]*0.7):,:]\n",
    "\n",
    "X = zh_en_train.iloc[:,:-1]\n",
    "Y = zh_en_train.iloc[:,-1]\n",
    "X_test = zh_en_test.iloc[:,:-1]\n",
    "Y_test = zh_en_test.iloc[:,-1]\n",
    "\n",
    "model_zh_en = LinearRegression()\n",
    "model_zh_en.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_zh_en.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-theme",
   "metadata": {},
   "source": [
    "### English into Chinese "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-portugal",
   "metadata": {},
   "source": [
    "As only the BLEU Corpus yields acceptable results, there's no need for a Regression for this language pair!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-routine",
   "metadata": {},
   "source": [
    "### English into Finish "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l_fmeasure = []\n",
    "fmeasure_1 = []\n",
    "l_precision = []\n",
    "recall_1 = []\n",
    "precision_1 = []\n",
    "\n",
    "\n",
    "for i in range(en_fi.shape[0]):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1'], use_stemmer=True)\n",
    "    scores = scorer.score(en_fi.loc[i,\"reference\"], en_fi.loc[i,\"translation\"])\n",
    "    l_fmeasure.append(scores[\"rougeL\"].fmeasure)\n",
    "    fmeasure_1.append(scores[\"rouge1\"].fmeasure)\n",
    "    l_precision.append(scores[\"rougeL\"].precision)\n",
    "    recall_1.append(scores[\"rouge1\"].recall)\n",
    "    precision_1.append(scores[\"rouge1\"].precision)\n",
    "    \n",
    "en_fi[\"l_fmeasure\"] = l_fmeasure\n",
    "en_fi[\"fmeasure_1\"] = fmeasure_1\n",
    "en_fi[\"l_precision\"] = l_precision\n",
    "en_fi[\"recall_1\"] = recall_1\n",
    "en_fi[\"precision_1\"] = precision_1\n",
    "\n",
    "en_fi_scores = en_fi[[\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"recall_1\", \"precision_1\", \"z-score\"]]\n",
    "\n",
    "for element in [\"l_fmeasure\", \"fmeasure_1\", \"l_precision\", \"recall_1\", \"precision_1\"]:\n",
    "    en_fi_scores[element] = zscore(en_fi_scores[element])\n",
    "    \n",
    "en_fi_train = en_fi_scores.iloc[:int(en_fi_scores.shape[0]*0.7),:]\n",
    "en_fi_test = en_fi_scores.iloc[int(en_fi_scores.shape[0]*0.7):,:]\n",
    "\n",
    "X = en_fi_train.iloc[:,:-1]\n",
    "Y = en_fi_train.iloc[:,-1]\n",
    "X_test = en_fi_test.iloc[:,:-1]\n",
    "Y_test = en_fi_test.iloc[:,-1]\n",
    "\n",
    "model_en_fi = LinearRegression()\n",
    "model_en_fi.fit(X,Y)\n",
    "\n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_en_fi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-character",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "collective-condition",
   "metadata": {},
   "source": [
    "# REGRESSION ON TOP OF ALL LEXICAL METRICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-passing",
   "metadata": {},
   "source": [
    "From this point, a consistent train dev split of 80:20 is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-gravity",
   "metadata": {},
   "source": [
    "### RU EN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_ = ru_en_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    ru_en_[element] = zscore(ru_en_[element])\n",
    "    \n",
    "    \n",
    "ru_en__train, ru_en__test = train_test_split(ru_en_, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "X = ru_en__train.iloc[:,1:]\n",
    "Y = ru_en__train.iloc[:,0]\n",
    "X_test = ru_en__test.iloc[:,1:]\n",
    "Y_test = ru_en__test.iloc[:,0] \n",
    "X_full = ru_en_.iloc[:,1:]\n",
    "Y_full = ru_en_.iloc[:,0] \n",
    "\n",
    "model_ru_en_ = LinearRegression()\n",
    "model_ru_en_.fit(X,Y)\n",
    "\n",
    "final_model_ru_en = LinearRegression()\n",
    "final_model_ru_en.fit(X_full, Y_full)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_ru_en_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-modeling",
   "metadata": {},
   "source": [
    "### DE EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_en_ = de_en_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])\n",
    "\n",
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    de_en_[element] = zscore(de_en_[element])\n",
    "    \n",
    "de_en__train, de_en__test = train_test_split(de_en_, test_size=0.2, random_state=0, shuffle=True)\n",
    "    \n",
    "\n",
    "X = de_en__train.iloc[:,1:]\n",
    "Y = de_en__train.iloc[:,0]\n",
    "X_test = de_en__test.iloc[:,1:]\n",
    "Y_test = de_en__test.iloc[:,0] \n",
    "\n",
    "model_de_en_ = LinearRegression()\n",
    "model_de_en_.fit(X,Y)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_de_en_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-murray",
   "metadata": {},
   "source": [
    "### CS EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_en_ = cs_en_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])\n",
    "\n",
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    cs_en_[element] = zscore(cs_en_[element])\n",
    "    \n",
    "cs_en__train, cs_en__test = train_test_split(cs_en_, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "X = cs_en__train.iloc[:,1:]\n",
    "Y = cs_en__train.iloc[:,0]\n",
    "X_test = cs_en__test.iloc[:,1:]\n",
    "Y_test = cs_en__test.iloc[:,0] \n",
    "X_full = cs_en_.iloc[:,1:]\n",
    "Y_full = cs_en_.iloc[:,0] \n",
    "\n",
    "model_cs_en_ = LinearRegression()\n",
    "model_cs_en_.fit(X,Y)\n",
    "\n",
    "final_model_cs_en = LinearRegression()\n",
    "final_model_cs_en.fit(X_full, Y_full)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_cs_en_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-beach",
   "metadata": {},
   "source": [
    "### ZH EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_en_ = zh_en_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])\n",
    "\n",
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    zh_en_[element] = zscore(zh_en_[element])\n",
    "    \n",
    "zh_en__train, zh_en__test = train_test_split(zh_en_, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "X = zh_en__train.iloc[:,1:]\n",
    "Y = zh_en__train.iloc[:,0]\n",
    "X_test = zh_en__test.iloc[:,1:]\n",
    "Y_test = zh_en__test.iloc[:,0] \n",
    "X_full = zh_en_.iloc[:,1:]\n",
    "Y_full = zh_en_.iloc[:,0] \n",
    "\n",
    "model_zh_en_ = LinearRegression()\n",
    "model_zh_en_.fit(X,Y)\n",
    "\n",
    "final_model_zh_en = LinearRegression()\n",
    "final_model_zh_en.fit(X_full, Y_full)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_zh_en_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-pixel",
   "metadata": {},
   "source": [
    "### EN ZH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_ = en_zh_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])\n",
    "\n",
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    en_zh_[element] = zscore(en_zh_[element])\n",
    "    \n",
    "en_zh__train, en_zh__test = train_test_split(en_zh_, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "X = en_zh__train.iloc[:,1:]\n",
    "Y = en_zh__train.iloc[:,0]\n",
    "X_test = en_zh__test.iloc[:,1:]\n",
    "Y_test = en_zh__test.iloc[:,0] \n",
    "\n",
    "model_en_zh_ = LinearRegression()\n",
    "model_en_zh_.fit(X,Y)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_en_zh_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-hybrid",
   "metadata": {},
   "source": [
    "### EN FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fi_ = en_fi_.drop(columns=[\"source\", \"reference\", \"translation\", \"avg-score\", \"annotators\"])\n",
    "\n",
    "for element in ['BLEU', 'BLEU_s', 'BLEU_c', 'ROUGE1_precision', 'ROUGE1_recall', 'ROUGE1_fmeasure', 'ROUGE2_precision', 'ROUGE2_recall', 'ROUGE2_fmeasure', 'ROUGEL_precision', 'ROUGEL_recall', 'ROUGEL_fmeasure']:\n",
    "    en_fi_[element] = zscore(en_fi_[element])\n",
    "    \n",
    "en_fi__train, en_fi__test = train_test_split(en_fi_, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "X = en_fi__train.iloc[:,1:]\n",
    "Y = en_fi__train.iloc[:,0]\n",
    "X_test = en_fi__test.iloc[:,1:]\n",
    "Y_test = en_fi__test.iloc[:,0] \n",
    "\n",
    "model_en_fi_ = LinearRegression()\n",
    "model_en_fi_.fit(X,Y)\n",
    "\n",
    "final_model_en_fi = LinearRegression()\n",
    "final_model_en_fi.fit(X_full, Y_full)\n",
    "    \n",
    "result = pd.DataFrame(Y_test)\n",
    "result[\"regression_values\"] = model_en_fi_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-chassis",
   "metadata": {},
   "source": [
    "## THE OVERALL BEST CORRELATIONS PER LANGUAGE PAIR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-kentucky",
   "metadata": {},
   "source": [
    "##### RU EN\n",
    "* Regression with all 12 metrics included (0.368)\n",
    "\n",
    "##### DE EN\n",
    "* ROUGE L Fmeasure Pearson (0.328)\n",
    "\n",
    "##### CS EN\n",
    "* Regression with all 12 metrics included (0.483)\n",
    "\n",
    "##### ZH EN\n",
    "* Regression with all 12 metrics included (0.360)\n",
    "\n",
    "##### EN ZH\n",
    "* BLEU Corpus Pearson (0.424)\n",
    "\n",
    "##### EN FI\n",
    "* ROUGE 1 Precision Pearson (0.549)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-bangkok",
   "metadata": {},
   "source": [
    "# Final Metric Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "#importing the data\n",
    "\n",
    "path_corpus_test = \"/Users/franz/Downloads/testset/\"\n",
    "\n",
    "ru_en_test = pd.read_csv(path_corpus_test + \"ru-en/scores.csv\")\n",
    "de_en_test = pd.read_csv(path_corpus_test + \"de-en/scores.csv\")\n",
    "cs_en_test = pd.read_csv(path_corpus_test + \"cs-en/scores.csv\")\n",
    "zh_en_test = pd.read_csv(path_corpus_test + \"zh-en/scores.csv\")\n",
    "en_zh_test = pd.read_csv(path_corpus_test + \"en-zh/scores.csv\")\n",
    "en_fi_test = pd.read_csv(path_corpus_test + \"en-fi/scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the nan\n",
    "ru_en_test.iloc[9191,1] = \"This is a dummy text, as the nan has to be filled!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the final functions of this metric\n",
    "\n",
    "def metric(element):\n",
    "    # detect the language pair contained in the dataframe\n",
    "    pair = detect(element.iloc[0,0]) + \"_\" + detect(element.iloc[0,1])\n",
    "    \n",
    "    predicted_scores = []\n",
    "    \n",
    "    #check if pair in list, if so, all 12 metrics need to be computed to feed to the respective model\n",
    "    if pair in [\"ru_en\", \"cs_en\", \"zh-cn_en\"]:\n",
    "        \n",
    "        all_12_df = pd.DataFrame()\n",
    "        \n",
    "        bleu_star_scores = []\n",
    "        bleu_sentence_scores = []\n",
    "        bleu_corpus_scores = []\n",
    "        r_1_p = []\n",
    "        r_1_r = []\n",
    "        r_1_f = []\n",
    "        r_2_p = []\n",
    "        r_2_r = []\n",
    "        r_2_f = []\n",
    "        r_l_p = []\n",
    "        r_l_r = []\n",
    "        r_l_f = []\n",
    "\n",
    "        \n",
    "        for i in range(element.shape[0]):\n",
    "            #calculating the bleu star scores for the translations in comparison to their respective reference\n",
    "            reference = element.loc[i,\"reference\"]\n",
    "            translation = element.loc[i,\"translation\"]\n",
    "            bleu_star_scores.append(BLEU_star_compact(reference, translation))\n",
    "            \n",
    "            #calculating the bleu sentence scores for the translations in comparison to their respective reference\n",
    "            reference = [element.loc[i,\"reference\"].split()]\n",
    "            translation = element.loc[i,\"translation\"].split()\n",
    "            bleu_sentence_scores.append(sentence_bleu(reference, translation,weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "            \n",
    "            #calculating the bleu corpus scores for the translations in comparison to their respective reference\n",
    "            reference = [element.loc[i,\"reference\"].split()]\n",
    "            translation = element.loc[i,\"translation\"].split()\n",
    "            while len(reference) < len(translation):\n",
    "                reference.append(\" \")\n",
    "            while len(reference) > len(translation):\n",
    "                translation.append(\" \")\n",
    "            bleu_corpus_scores.append(corpus_bleu(reference, translation))\n",
    "        \n",
    "            #calculating all the ROUGE scores for the translations in comparison to their respective reference\n",
    "            scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1', 'rouge2'], use_stemmer=True)\n",
    "            scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "            \n",
    "            r_1_p.append(scores[\"rouge1\"].precision)\n",
    "            r_1_r.append(scores[\"rouge1\"].recall)\n",
    "            r_1_f.append(scores[\"rouge1\"].fmeasure)\n",
    "            \n",
    "            r_2_p.append(scores[\"rouge2\"].precision)\n",
    "            r_2_r.append(scores[\"rouge2\"].recall)\n",
    "            r_2_f.append(scores[\"rouge2\"].fmeasure)\n",
    "            \n",
    "            r_l_p.append(scores[\"rougeL\"].precision)\n",
    "            r_l_r.append(scores[\"rougeL\"].recall)\n",
    "            r_l_f.append(scores[\"rougeL\"].fmeasure)\n",
    "            \n",
    "        all_12_df[\"BLEU\"] = bleu_star_scores\n",
    "        all_12_df[\"BLEU_s\"] = bleu_sentence_scores\n",
    "        all_12_df[\"BLEU_c\"] = bleu_corpus_scores\n",
    "        all_12_df[\"ROUGE1_precision\"] = r_1_p\n",
    "        all_12_df[\"ROUGE1_recall\"] = r_1_r\n",
    "        all_12_df[\"ROUGE1_fmeasure\"] = r_1_f\n",
    "        all_12_df[\"ROUGE2_precision\"] = r_2_p\n",
    "        all_12_df[\"ROUGE2_recall\"] = r_2_r\n",
    "        all_12_df[\"ROUGE2_fmeasure\"] = r_2_f\n",
    "        all_12_df[\"ROUGEL_precision\"] = r_l_p\n",
    "        all_12_df[\"ROUGEL_recall\"] = r_l_r\n",
    "        all_12_df[\"ROUGEL_fmeasure\"] = r_l_f\n",
    "            \n",
    "        #the following part to be changed to the full model if applied to the test set, as it will yiel better results\n",
    "        if pair == \"ru_en\":\n",
    "            return model_ru_en_.predict(all_12_df)\n",
    "        elif pair == \"cs_en\":\n",
    "            return model_cs_en_.predict(all_12_df)\n",
    "        elif pair == \"zh-cn_en\":\n",
    "            return model_zh_en_.predict(all_12_df)\n",
    "                \n",
    "    elif pair in [\"de_en\", \"en_fi\"]:\n",
    "        for i in range(element.shape[0]):\n",
    "            scorer = rouge_scorer.RougeScorer(['rougeL', 'rouge1', 'rouge2'], use_stemmer=True)\n",
    "            scores = scorer.score(element.loc[i,\"reference\"], element.loc[i,\"translation\"])\n",
    "            if pair == \"de_en\":\n",
    "                predicted_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "            elif pair == \"en_fi\":\n",
    "                predicted_scores.append(scores[\"rouge1\"].precision)\n",
    "        return predicted_scores\n",
    "                \n",
    "    elif pair == \"en_zh-cn\":\n",
    "        for i in range(element.shape[0]):\n",
    "            reference = [element.loc[i,\"reference\"].split()]\n",
    "            translation = element.loc[i,\"translation\"].split()\n",
    "            while len(reference) < len(translation):\n",
    "                reference.append(\" \")\n",
    "            while len(reference) > len(translation):\n",
    "                translation.append(\" \")\n",
    "            predicted_scores.append(corpus_bleu(reference, translation))\n",
    "        return predicted_scores\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-origin",
   "metadata": {},
   "source": [
    "## Testing on Development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the data\n",
    "\n",
    "path_corpus = \"/Users/franz/Desktop/TM Project/corpus/\"\n",
    "\n",
    "ru_en__ = pd.read_csv(path_corpus + \"ru-en/scores.csv\")\n",
    "de_en__ = pd.read_csv(path_corpus + \"de-en/scores.csv\")\n",
    "cs_en__ = pd.read_csv(path_corpus + \"cs-en/scores.csv\")\n",
    "zh_en__ = pd.read_csv(path_corpus + \"zh-en/scores.csv\")\n",
    "en_zh__ = pd.read_csv(path_corpus + \"en-zh/scores.csv\")\n",
    "en_fi__ = pd.read_csv(path_corpus + \"en-fi/scores.csv\")\n",
    "\n",
    "\n",
    "ru_en__ = ru_en__[ru_en__.index.isin(ru_en__test.index.tolist())].reset_index(drop=True)\n",
    "de_en__ = de_en__[de_en__.index.isin(de_en__test.index.tolist())].reset_index(drop=True)\n",
    "cs_en__ = cs_en__[cs_en__.index.isin(cs_en__test.index.tolist())].reset_index(drop=True)\n",
    "zh_en__ = zh_en__[zh_en__.index.isin(zh_en__test.index.tolist())].reset_index(drop=True)\n",
    "en_zh__ = en_zh__[en_zh__.index.isin(en_zh__test.index.tolist())].reset_index(drop=True)\n",
    "en_fi__ = en_fi__[en_fi__.index.isin(en_fi__test.index.tolist())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-newton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ru_en__[\"scores\"] = metric(ru_en__)\n",
    "de_en__[\"scores\"] = metric(de_en__)\n",
    "cs_en__[\"scores\"] = metric(cs_en__)\n",
    "zh_en__[\"scores\"] = metric(zh_en__)\n",
    "en_zh__[\"scores\"] = metric(en_zh__)\n",
    "en_fi__[\"scores\"] = metric(en_fi__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "k = []\n",
    "for element in [ru_en__, de_en__, cs_en__, zh_en__, en_zh__, en_fi__]:\n",
    "    p.append(element.corr().iloc[3,0])\n",
    "    k.append(element.corr(method=\"kendall\").iloc[3,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final scores for the dev set\n",
    "r_lex = pd.DataFrame([p, k], columns = [\"ru_en\", \"de_en\",\"cs_en\",\"zh_en\", \"en_zh\", \"en_fi\"], index=[\"Pearson\", \"Kendall\"])\n",
    "r_lex[\"mean\"] = r_lex.T.mean()\n",
    "r_lex \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-interference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-editor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "subjective-vatican",
   "metadata": {},
   "source": [
    "## Exporting scores for the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_test[\"scores\"] = metric(ru_en_test)\n",
    "de_en_test[\"scores\"] = metric(de_en_test)\n",
    "cs_en_test[\"scores\"] = metric(cs_en_test)\n",
    "zh_en_test[\"scores\"] = metric(zh_en_test)\n",
    "en_zh_test[\"scores\"] = metric(en_zh_test)\n",
    "en_fi_test[\"scores\"] = metric(en_fi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_en_test.to_csv(path_corpus_test + \"ru-en/scores_added.csv\")\n",
    "de_en_test.to_csv(path_corpus_test + \"de-en/scores_added.csv\")\n",
    "cs_en_test.to_csv(path_corpus_test + \"cs-en/scores_added.csv\")\n",
    "zh_en_test.to_csv(path_corpus_test + \"zh-en/scores_added.csv\")\n",
    "en_zh_test.to_csv(path_corpus_test + \"en-zh/scores_added.csv\")\n",
    "en_fi_test.to_csv(path_corpus_test + \"en-fi/scores_added.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-registration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-closer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-collins",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-religion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-peace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-brother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-first",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-nashville",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
